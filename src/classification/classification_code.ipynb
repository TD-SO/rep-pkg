{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports --StackOverflow csv data dump not included--\n",
    "import pandas as pd\n",
    "import re\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from sklearn import svm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.collocations import *\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "#Processing methods\n",
    "\n",
    "#distinct number of instances\n",
    "def intersection_without_dup(body, words,postid,feature_container): \n",
    "    body_set = set(body) \n",
    "    words_set = set(words) \n",
    "    inter_set = set()\n",
    "    if (body_set & words_set): \n",
    "        #print(body_set & words_set)\n",
    "        inter_set = body_set & words_set\n",
    "        feature_container[postid] = len(inter_set)\n",
    "    \n",
    "\n",
    "    #total number of instances\n",
    "def intersection_with_dup(body_list, words_list,postid,feature_container): \n",
    "    inter_list = [value for value in body_list if value in words_list] \n",
    "    if (len(inter_list)>0):\n",
    "        feature_container[postid] = len(inter_list)\n",
    "   \n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    punctuation = [',', '.','@','``',\"''\",'#',\"'\",'&','...',':',';']\n",
    "    word_tokens=word_tokenize(text)\n",
    "    result = []\n",
    "    for token in word_tokens:\n",
    "        result.append(token.lower())\n",
    "        \n",
    "    filtered_result = [w for w in result if not w in stop_words]\n",
    "    ffiltered_result = [w for w in filtered_result if not w in punctuation]\n",
    "    fffiltered_result = [w for w in ffiltered_result if not len(w)<3]\n",
    "    return fffiltered_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the Stackoverflow data dump for mentions of technical debt in post body, title or tags.\n",
    "\n",
    "\n",
    "def process_pd(merged_pd):\n",
    "    \n",
    "    merged_pd=pd.DataFrame(np.vstack([merged_pd.columns, merged_pd]))\n",
    "    merged_pd=merged_pd.rename(columns={0: 'PostId',1: 'PostTypeId', 3: 'ParentId',4: 'Creation',6:'Score', 7:'ViewCount', 8: 'Body', 15: 'Title', 16: 'Tags', 17:'AnswerCount', 18:'CommentCount', 19: 'FavoriteCount'})\n",
    "    mask = (merged_pd['PostTypeId'] == 2.0) & (merged_pd['ParentId'] != 0)\n",
    "    merged_pd['PostId'][mask] = merged_pd['ParentId']\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('&lt;p&gt;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('&lt;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('&gt;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('&xA;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('/p&xA;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('/ul','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('/li','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('h1','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('h2','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('h3','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('/strong','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('&quot;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('/p&#xA;&#xA;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace(' &#xA;','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].fillna('')\n",
    "    merged_pd['Title'] = merged_pd['Title'].str.replace('&quot;','')\n",
    "    merged_pd['ParentId'] = merged_pd['ParentId'].fillna('')\n",
    "    merged_pd['Title'] = merged_pd['Title'].fillna('')\n",
    "    merged_pd['Tags'] = merged_pd['Tags'].str.replace('&lt;p&gt;','')\n",
    "    merged_pd['Tags'] = merged_pd['Tags'].str.replace('&lt;',' ')\n",
    "    merged_pd['Tags'] = merged_pd['Tags'].str.replace('&gt;',' ')\n",
    "    merged_pd['Tags'] = merged_pd['Tags'].str.replace('&xA;',' ')\n",
    "    merged_pd['Tags'] = merged_pd['Tags'].str.replace('/p&xA;','')\n",
    "    merged_pd['Tags'] = merged_pd['Tags'].fillna('')\n",
    "    merged_pd['Score'] = merged_pd['Score'].fillna(0)\n",
    "    merged_pd['ViewCount'] = merged_pd['ViewCount'].fillna(0)\n",
    "    merged_pd['AnswerCount'] = merged_pd['AnswerCount'].fillna(0)\n",
    "    merged_pd['CommentCount'] = merged_pd['CommentCount'].fillna(0)\n",
    "    merged_pd['FavoriteCount'] = merged_pd['FavoriteCount'].fillna(0)\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('href=','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('strongem','')\n",
    "    merged_pd['Body'] = merged_pd['Body'].str.replace('blockquote','')\n",
    "\n",
    "    return merged_pd\n",
    "\n",
    "def check_pd_body(merged_pd):\n",
    "    \n",
    "    return merged_pd[merged_pd['Body'].str.contains('technical debt|Technical debt|Technical Debt|technical debts|Technical debts|technical-debt|tech debt|tech-debt|techdebt')]\n",
    "\n",
    "def check_pd_title(merged_pd):\n",
    "    \n",
    "    return merged_pd[merged_pd['Title'].str.contains('technical debt|Technical debt|Technical Debt|technical debts|Technical debts|technical-debt|tech debt|tech-debt|techdebt')]\n",
    "\n",
    "def check_pd_tags(merged_pd):\n",
    "    \n",
    "    return merged_pd[merged_pd['Tags'].str.contains('technical-debt|tech-debt|techdebt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the parts after translating the stack overflow dump from xml to csv\n",
    "\n",
    "names=[]\n",
    "\n",
    "\n",
    "for name in sorted(glob('posts2_csv/*')):\n",
    "    names.append(name)\n",
    "    \n",
    "def query_tech_debt(df):\n",
    "    newDF = pd.DataFrame()\n",
    "    newDF_title = check_pd_title(process_pd(df))\n",
    "    newDF_body = check_pd_body(process_pd(df))\n",
    "    newDF_tags = check_pd_tags(process_pd(df))\n",
    "    pids=[]\n",
    "    df = process_pd(df)\n",
    "\n",
    "    \n",
    "    if(newDF_title.empty == False) or (newDF_body.empty == False) or (newDF_tags.empty == False):\n",
    "        for pid in newDF_title['PostId']:\n",
    "            pids.append(pid)\n",
    "        for pid in newDF_body['PostId']:\n",
    "            pids.append(pid)\n",
    "        for pid in newDF_tags['PostId']:\n",
    "            pids.append(pid)\n",
    "        for pid in pids:\n",
    "            newDF=newDF.append(df.loc[df['PostId'] == pid, ['PostId','PostTypeId','Title','Body','Score','ViewCount','CommentCount',  'FavoriteCount']])\n",
    "    \n",
    "   \n",
    "    return newDF.drop_duplicates(keep='first')\n",
    "        \n",
    "    \n",
    "       \n",
    "final_query_df=pd.DataFrame()\n",
    "title_df=pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for n in names[2000:]:\n",
    "    read_df=pd.read_csv(n)\n",
    "    final_query_df=final_query_df.append(query_tech_debt(read_df))\n",
    "    title_df=title_df.append(process_pd(read_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=final_query_df['PostTypeId'] == 1  \n",
    "final_query_df=final_query_df[mask]\n",
    "title_df= title_df[['PostId','ParentId', 'Title']]\n",
    "\n",
    "\n",
    "title_df=title_df.loc[title_df['ParentId'] == '', ['Title','PostId']]\n",
    "title_df= title_df.iloc[1:]\n",
    "title_df['PostId']=title_df['PostId'].fillna(0)\n",
    "title_df['PostId']=title_df['PostId'].astype(int)\n",
    "title_df=title_df.set_index('PostId')\n",
    "title_df = title_df.loc[~title_df.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "final_query_df['PostId']=final_query_df['PostId'].fillna(0)\n",
    "final_query_df['PostId']=final_query_df['PostId'].astype(int)\n",
    "final_query_df=final_query_df.set_index('PostId')\n",
    "\n",
    "#add titles to questions and answers\n",
    "for ind in final_query_df.index:\n",
    "            if(ind in final_query_df.index.values and ind in title_df.index.values):\n",
    "                final_query_df.loc[[ind], 'Title']= title_df.loc[[ind], 'Title']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDR CLASSIFIER FEATURES\n",
    "\n",
    "\n",
    "# loading the tech debt labeled dataset\n",
    "tech_labeled_data=pd.read_csv(\"tech_debt_labeled.csv\")\n",
    "tech_labeled_body_nocode = pd.read_csv(\"query_nocode.csv\")\n",
    "tech_labeled_data['Body'] = tech_labeled_body_nocode['Body']\n",
    "tech_labeled_data['TDTYPE'] = tech_labeled_data['TDTYPE'].fillna('')\n",
    "tech_labeled_data['URGENCY'] = tech_labeled_data['URGENCY'].fillna('0')\n",
    "\n",
    "\n",
    "\n",
    "#make new dictionaries for features\n",
    "#tokens\n",
    "body_tech_labeled_bigrams= defaultdict()\n",
    "title_tech_labeled_bigrams= defaultdict()\n",
    "body_tech_labeled_words= defaultdict()\n",
    "title_tech_labeled_words= defaultdict()\n",
    "\n",
    "#frequencies for bodies: distinct, total and all words\n",
    "body_tech_labeled_bigrams_total = defaultdict()\n",
    "body_tech_labeled_words_total=defaultdict()\n",
    "body_tech_labeled_bigrams_distinct = defaultdict()\n",
    "body_tech_labeled_words_distinct=defaultdict()\n",
    "body_total_tech_words = defaultdict()\n",
    "\n",
    "#frequencies for titles: distinct, total and all words\n",
    "title_tech_labeled_bigrams_total = defaultdict()\n",
    "title_tech_labeled_bigrams_distinct = defaultdict()\n",
    "title_tech_labeled_words_total=defaultdict()\n",
    "title_tech_labeled_words_distinct=defaultdict()\n",
    "title_total_tech_words = defaultdict()\n",
    "\n",
    "\n",
    "\n",
    "#find bigrams and unigrams in body and title of posts and associate with postID\n",
    "for ind in tech_labeled_data.index: \n",
    "    body_tech_labeled_bigrams[tech_labeled_data['PostId'][ind]] = list(nltk.bigrams(preprocess(str(tech_labeled_data['Body'][ind]))))\n",
    "    body_tech_labeled_words[tech_labeled_data['PostId'][ind]] = preprocess(str(tech_labeled_data['Body'][ind]))\n",
    "    \n",
    "for ind in tech_labeled_data.index: \n",
    "    title_tech_labeled_bigrams[tech_labeled_data['PostId'][ind]] = list(nltk.bigrams(preprocess(str(tech_labeled_data['Title'][ind]))))\n",
    "    title_tech_labeled_words[tech_labeled_data['PostId'][ind]] = preprocess(str(tech_labeled_data['Title'][ind]))\n",
    "    \n",
    "    \n",
    "for postid,bgrm in body_tech_labeled_bigrams.items():\n",
    "        intersection_with_dup(bgrm,debt_list,postid,body_tech_labeled_bigrams_total)\n",
    "        intersection_without_dup(bgrm,debt_list,postid,body_tech_labeled_bigrams_distinct)\n",
    "        \n",
    "for postid,word in  body_tech_labeled_words.items():\n",
    "        intersection_with_dup(word,word_freq,postid,body_tech_labeled_words_total)\n",
    "        intersection_without_dup(word,word_freq,postid,body_tech_labeled_words_distinct)\n",
    "        body_total_tech_words[postid] = word\n",
    "        \n",
    "for postid,bgrm in title_tech_labeled_bigrams.items():\n",
    "        intersection_with_dup(bgrm,debt_list,postid,title_tech_labeled_bigrams_total)\n",
    "        intersection_without_dup(bgrm,debt_list,postid,title_tech_labeled_bigrams_distinct)\n",
    "        \n",
    "for postid,word in  title_tech_labeled_words.items():\n",
    "        intersection_with_dup(word,word_freq,postid,title_tech_labeled_words_total)\n",
    "        intersection_without_dup(word,word_freq,postid,title_tech_labeled_words_distinct)\n",
    "        \n",
    "        \n",
    "for postid,words in body_total_tech_words.items():\n",
    "     body_total_tech_words[postid] = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "tfidf_body = tfidf_vectorizer.fit_transform(tech_labeled_data['Body']).toarray()\n",
    "tfidf_title= tfidf_vectorizer.fit_transform(tech_labeled_data['Title']).toarray()\n",
    "tech_labeled_data['TFIDFBODY'] = [np.sum(arr) for arr in tfidf_body]\n",
    "tech_labeled_data['TFIDFTITLE'] = [np.sum(arr) for arr in tfidf_title]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features added to csv dataframe\n",
    "\n",
    "bbt=pd.DataFrame(body_tech_labeled_bigrams_total.items(), columns=['PostId', 'BodyTotalTechBigrams'])\n",
    "bbd=pd.DataFrame(body_tech_labeled_bigrams_distinct.items(), columns=['PostId', 'BodyDistinctTechBigrams'])\n",
    "btw = pd.DataFrame(body_tech_labeled_words_total.items(), columns=['PostId', 'BodyTotalTechWords'])\n",
    "bdw = pd.DataFrame(body_tech_labeled_words_distinct.items(), columns=['PostId', 'BodyDistinctTechWords'])\n",
    "bwt = pd.DataFrame(body_total_tech_words.items(), columns=['PostId', 'TotalBodyWords'])\n",
    "tbt=pd.DataFrame(title_tech_labeled_bigrams_total.items(), columns=['PostId', 'TitleTotalTechBigrams'])\n",
    "tbd=pd.DataFrame(title_tech_labeled_bigrams_distinct.items(), columns=['PostId', 'TitleDistinctTechBigrams'])\n",
    "\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, bbt, how='outer', on='PostId')\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, bbd, how='outer', on='PostId')\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, btw, how='outer', on='PostId')\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, bdw, how='outer', on='PostId')\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, bwt, how='outer', on='PostId')\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, tbt, how='outer', on='PostId')\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, tbd, how='outer', on='PostId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fill outer merge nans\n",
    "tech_labeled_data['BodyTotalTechBigrams'] = tech_labeled_data['BodyTotalTechBigrams'].fillna('0')\n",
    "tech_labeled_data['BodyDistinctTechBigrams'] = tech_labeled_data['BodyDistinctTechBigrams'].fillna('0')\n",
    "tech_labeled_data['BodyTotalTechWords'] = tech_labeled_data['BodyTotalTechWords'].fillna('0')\n",
    "tech_labeled_data['BodyDistinctTechWords'] = tech_labeled_data['BodyDistinctTechWords'].fillna('0')\n",
    "tech_labeled_data['TotalBodyWords'] = tech_labeled_data['TotalBodyWords'].fillna('0')\n",
    "tech_labeled_data['TitleTotalTechBigrams'] = tech_labeled_data['TitleTotalTechBigrams'].fillna('0')\n",
    "tech_labeled_data['TitleDistinctTechBigrams'] = tech_labeled_data['TitleDistinctTechBigrams'].fillna('0')\n",
    "tech_labeled_data['TDR'] = tech_labeled_data['TDR'].fillna('0')\n",
    "tech_labeled_data['TDR'] = tech_labeled_data['TDR'].astype(int)\n",
    "tech_labeled_data['TFIDFBODY']=tech_labeled_data['TFIDFBODY'].fillna(0)\n",
    "tech_labeled_data['TFIDFTITLE']=tech_labeled_data['TFIDFTITLE'].fillna(0)\n",
    "tech_labeled_data['TFIDFBODY']=tech_labeled_data['TFIDFBODY'].astype(int)\n",
    "tech_labeled_data['TFIDFTITLE']=tech_labeled_data['TFIDFTITLE'].astype(int)\n",
    "\n",
    "#Add scores and comments\n",
    "scores_comments = pd.read_csv('scores_comments.csv')\n",
    "scores_comments = scores_comments[['PostId','Score', 'CommentCount']]\n",
    "tech_labeled_data=pd.merge(tech_labeled_data, scores_comments, how='outer', on='PostId')\n",
    "tech_labeled_data['Score'] = tech_labeled_data['Score'].fillna(0)\n",
    "tech_labeled_data['Score'] = tech_labeled_data['Score'].astype(int)\n",
    "tech_labeled_data['CommentCount'] = tech_labeled_data['CommentCount'].fillna(0)\n",
    "tech_labeled_data['CommentCount'] = tech_labeled_data['CommentCount'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDR CLASSIFICATION\n",
    "import joblib\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Splitting the data into independent and dependent variables\n",
    "X = tech_labeled_data[['TFIDFBODY','BodyDistinctTechBigrams','BodyDistinctTechWords','Score']]\n",
    "y = tech_labeled_data['TDR']\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = RandomForestClassifier(bootstrap=True,max_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0,max_depth=24, random_state=42,criterion='entropy',min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "# Create the model on train dataset\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    " \n",
    "# Calculate the accuracy\n",
    "print(accuracy_score(y_test, model.predict(X_test), normalize=True)*100)\n",
    "\n",
    "# KFold Cross Validation approach\n",
    "kf = KFold(n_splits=10,shuffle=False)\n",
    "kf.split(X)    \n",
    "     \n",
    "# Initialize the accuracy of the models to blank list. The accuracy of each model will be appended to this list\n",
    "accuracy_model = []\n",
    " \n",
    "# Iterate over each train-test split\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split train-test\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # Train the model\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    # Append to accuracy_model the accuracy of the model\n",
    "    accuracy_model.append(accuracy_score(y_test, model.predict(X_test), normalize=True)*100)\n",
    "\n",
    "# Print the accuracy    \n",
    "print(accuracy_model)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize accuracy for each iteration\n",
    " \n",
    "scores = pd.DataFrame(accuracy_model,columns=['Scores'])\n",
    " \n",
    "sns.set(style=\"white\", rc={\"lines.linewidth\": 3})\n",
    "sns.barplot(x=['Iter1','Iter2','Iter3','Iter4','Iter5','Iter6','Iter7','Iter8','Iter9','Iter10'],y=\"Scores\",data=scores)\n",
    "plt.show()\n",
    "sns.set()\n",
    "print(confusion_matrix(y_test,model.predict(X_test)))\n",
    "print(classification_report(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Technical Debt Urgency multiclass\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "\n",
    "X = tdru[['TFIDFBODY','BodyDistinctTechBigrams','BodyDistinctTechWords']]\n",
    "y=tdru['URGENCY']\n",
    "\n",
    "over = SMOTE(random_state=12,sampling_strategy='all')\n",
    "#over = RandomOverSampler(random_state=0,sampling_strategy='all')\n",
    "X, y = over.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(bootstrap=True,max_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0,max_depth=24, random_state=42,criterion='entropy',min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "# Create the model on train dataset\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    " \n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, model.predict(X_test), target_names=['None','Very mild','Mild','Moderate','Severe','Very Severe']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URGENCY TRINARY\n",
    "#import SMOTE\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "\n",
    "tdru['URGENCY'] = tdru['URGENCY'].map({1:0,2:1, 3:1, 4:2,5:2})\n",
    "tdru['URGENCY']=tdru['URGENCY'].fillna(0)\n",
    "tdru['URGENCY']=tdru['URGENCY'].astype(int)\n",
    "\n",
    "#Score can also be added\n",
    "X = tdru[['TFIDFBODY','BodyDistinctTechBigrams','BodyDistinctTechWords']]\n",
    "y=tdru['URGENCY']\n",
    "\n",
    "over = SMOTE(sampling_strategy='all')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_train, y_train= over.fit_resample(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier(bootstrap=True,max_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0,max_depth=70, random_state=0,criterion='entropy',min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "# Create the model on train dataset\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    " \n",
    "# Calculate the accuracy\n",
    "\n",
    "print(metrics.classification_report(y_test, model.predict(X_test), target_names=['None-Very mild','Mild-Moderate','Severe-Very Severe']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#factorizing the td types and combining over/undersampling\n",
    "\n",
    "\n",
    "#Score feature can also be added\n",
    "XX = tdrs[['TFIDFBODY','BodyDistinctTechBigrams','BodyDistinctTechWords']]\n",
    "yy = tdrs['TDTYPENUM']\n",
    "\n",
    "over_smote = SMOTE(sampling_strategy='auto')\n",
    "\n",
    "#TDTYPE RANDOM FOREST\n",
    "\n",
    "XX_train, XX_test, yy_train,yy_test = train_test_split(XX, yy, test_size = 0.33, random_state = 0)\n",
    "\n",
    "XX_train, yy_train= over_smote.fit_resample(XX_train, yy_train)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(bootstrap=True,max_features=None, max_leaf_nodes=None,min_impurity_decrease=0.0,max_depth=70, random_state=0,criterion='entropy',min_samples_leaf=1, min_samples_split=2)\n",
    "\n",
    "# Create the model on train dataset\n",
    "\n",
    "model = clf.fit(XX_train, yy_train)\n",
    " \n",
    "\n",
    "print(metrics.classification_report(yy_test, model.predict(XX_test), target_names=[t for t in types]))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
